# Review-and-Outlook-of-Shared-Multi-Modal-Trustworthy-Human-Machine-Interaction-Research[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/XingfuCao/Review-and-Outlook-of-Shared-Multi-Modal-Trustworthy-Human-Machine-Interaction-Research)

A curated list of research papers in human-machine interaction.

## üí¨ News
**[2023/04/2]**: Create this repository.

<!-- 1. First Author. **Paper Name**. Conf. [[Paper]]() [[Code]]() [[Website]]() -->

## multimodal human-robot interaction<br>
#### 2021
1. Yixin Chen, Qing Li, et al. **YouRefIt: Embodied Reference Understanding with Language and Gesture**. 	ICCV 2021.[[Paper]](https://arxiv.org/abs/2109.03413) 
2. Paul Pu Liang, Yiwei Lyu, et al. **MultiBench: Multiscale Benchmarks for Multimodal Representation Learning**. NeurIPS 2021.[[Paper]](https://arxiv.org/abs/2107.07502) [[Code1]](https://github.com/pliang279/MultiBench) [[Code2]](https://github.com/pliang279/awesome-multimodal-ml)
#### 2022
1. Tim Salzmann, Marco Pavone, et al. **Motron: Multimodal Probabilistic Human Motion Forecasting**. 	CVPR 2022. [[Paper]](https://arxiv.org/abs/2203.04132) [[Code]](https://github.com/TUM-AAS/motron-cvpr22)
2. Robert L. Wilson, Daniel Browne, et al. **A Virtual Reality Simulation Pipeline for Online Mental Workload Modeling**. VR 2022.[[Paper]](https://arxiv.org/abs/2111.03977) 
#### 2023
1. Eley Ng, Ziang Liu, et al. **It Takes Two: Learning to Plan for Human-Robot Cooperative Carrying**. ICRA 2023. [[Paper]](https://arxiv.org/abs/2209.12890) [[Code1]](https://github.com/eleyng/table-carrying-ai)[[Code2]](https://github.com/eleyng/cooperative_planner)

## multimodal fusion<br>
#### 2020
1. Jie Lei, Licheng Yu, et al. **TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval**. 	ECCV 2020. [[Paper]](https://arxiv.org/abs/2001.09099) [[Code1]](https://github.com/jayleicn/TVRetrieval) [[code2]](https://github.com/jayleicn/TVCaption)
2. Ronghang Hu, Amanpreet Singh, et al. **Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA**. 	CVPR 2020. [[Paper]](https://arxiv.org/abs/1911.06258) [[Code]](https://github.com/adlnlp/attention_vl)
3. Mahdi Abavisani, Liwei Wu, et al. **Multimodal Categorization of Crisis Events in Social Media**. CVPR 2020.[[Paper]](https://arxiv.org/abs/2004.04917) [[Code]](https://github.com/PaulCCCCCCH/Multimodal-Categorization-of-Crisis-Events-in-Social-Media)
4. Shaofei Huang, Tianrui Hui, et al.**Referring Image Segmentation via Cross-Modal Progressive Comprehension**. CVPR 2020. [[Paper]](https://arxiv.org/abs/2010.00514) [[Code]](https://github.com/spyflying/CMPC-Refseg)
5. Sijie Mai, Haifeng Hu, et al. **Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion**. AAAI 2020. [[Paper]](https://arxiv.org/abs/1911.07848) [[Code]](https://github.com/TmacMai/ARGF_multimodal_fusion)
#### 2021
1. Kranti Kumar Parida, Siddharth Srivastava, et al. **Beyond Image to Depth: Improving Depth Prediction using Echoes**. CVPR 2021. [[Paper]](https://arxiv.org/abs/2103.08468) [[Code]](https://github.com/krantiparida/beyond-image-to-depth)
2. Yapeng Tian, Chenliang Xu.**Can audio-visual integration strengthen robustness under multimodal attacks?**. 	CVPR 2021. [[Paper]](https://arxiv.org/abs/2104.02000) [[Code]](https://github.com/YapengTian/AV-Robustness-CVPR21)
3. Jiapeng Wang, Chongyu Liu, et al. **Towards Robust Visual Information Extraction in Real World: New Dataset and Novel Solution**. AAAI 2021. [[Paper]](https://arxiv.org/abs/2102.06732) [[Code]](https://github.com/HCIILAB/EPHOIE)
4. Peng Qi, Juan Cao, et al. **Improving Fake News Detection by Using an Entity-enhanced Framework to Fuse Diverse Multimodal Clues**. MM 2021. [[Paper]](https://arxiv.org/abs/2108.10509)
5. Fangneng Zhan, Yingchen Yu, et al. **Multimodal Image Synthesis and Editing: A Survey**. TPAMI 2021. [[Paper]](https://arxiv.org/abs/2112.13592) [[Code]](https://github.com/fnzhan/mise)
#### 2022
1. Dewang Hou, Yuanyuan Du,et al. **Learning an Efficient Multimodal Depth Completion Model**. ECCV 2022. [[Paper]](https://arxiv.org/abs/2208.10771) [[Code]](https://github.com/dwhou/emdc-pytorch) 
2. Tim Salzmann, Marco Pavone, . **Motron: Multimodal Probabilistic Human Motion Forecasting**. CVPR 2022. [[Paper]](https://arxiv.org/abs/2203.04132) [[Code]](https://github.com/TUM-AAS/motron-cvpr22) 
3. Zi-Yi Dou, Yichong Xu, et al. **An Empirical Study of Training End-to-End Vision-and-Language Transformers**. CVPR 2022. [[Paper]](https://arxiv.org/abs/2111.02387) [[Code]](https://github.com/zdou0830/meter)
4. Matthew Walmer, Karan Sikka, et al. **Dual-Key Multimodal Backdoors for Visual Question Answering**. CVPR 2022. [[Paper]](https://arxiv.org/abs/2112.07668) [[Code]](https://github.com/SRI-CSL/TrinityMultimodalTrojAI)
5. Xiaokang Peng, Yake Wei, et al. **Balanced Multimodal Learning via On-the-fly Gradient Modulation**. CVPR 2022. [[Paper]](https://arxiv.org/abs/2203.15332) [[Code]](https://github.com/gewu-lab/ogm-ge_cvpr2022)
6. Momal Ijaz, Renato Diaz, et al. **Multimodal Transformer for Nursing Activity Recognition**. CVPR 2022. [[Paper]](https://arxiv.org/abs/2204.04564) [[Code]](https://github.com/momilijaz96/mmt_for_ncrc)
7. Yikai Wang, Xinghao Chen, et al. **Multimodal Token Fusion for Vision Transformers**. 	CVPR 2022. [[Paper]](https://arxiv.org/abs/2204.08721) [[Code1]](https://github.com/yikaiw/TokenFusion)[[Code2]](https://github.com/huawei-noah/noah-research/tree/master/TokenFusion)[[Code3]](https://github.com/mindspore-ai/models/tree/master/research/cv/TokenFusion)
8. Yihang Yin, Siyu Huang, et al. **BM-NAS: Bilevel Multimodal Neural Architecture Search**. AAAI 2022. [[Paper]](https://arxiv.org/abs/2104.09379) [[Code]](https://github.com/Somedaywilldo/BM-NAS)
9. Hanlei Zhang, Hua Xu, et al. **MIntRec: A New Dataset for Multimodal Intent Recognition**. MM 2022. [[Paper]](https://arxiv.org/abs/2209.04355) [[Code]](https://github.com/thuiar/mintrec)
10. Yikai Wang, Fuchun Sun, et al. **Channel Exchanging Networks for Multimodal and Multitask Dense Image Prediction**. TPAMI 2022. [[Paper]](https://arxiv.org/abs/2112.02252) [[Code]](https://github.com/yikaiw/CEN)
#### 2023
1. Yue Wang, Jinlong Peng, et al. **Multimodal Industrial Anomaly Detection via Hybrid Fusion**. CVPR 2023. [[Paper]](https://arxiv.org/abs/2303.00601) [[Code]](https://github.com/nomewang/m3dm)
2. Jiaming Zhang, Ruiping Liu, et al. **Delivering Arbitrary-Modal Semantic Segmentation**. CVPR 2023. [[Paper]](https://arxiv.org/abs/2303.01480) [[Code]](https://github.com/jamycheung/DELIVER)
3. Lei Wang, Jiabang He, et al. **Alignment-Enriched Tuning for Patch-Level Pre-trained Document Image Models**. AAAI 2023. [[Paper]](https://arxiv.org/abs/2211.14777) [[Code]](https://github.com/maehcm/aet)
4. Qiying Yu, Yang Liu, et al. **Multimodal Federated Learning via Contrastive Representation Ensemble**. ICLR 2023. [[Paper]](https://arxiv.org/abs/2302.08888) [[Code]](https://github.com/flair-thu/creamfl)

## shared awareness cues / shared awareness interaction <br>
#### 2020
1. Chenglong Li, Lei Liu, et al. **Challenge-Aware RGBT Tracking**. ECCV 2020. [[Paper]](https://arxiv.org/abs/2007.13143) 
#### 2021
1. Jongjin Park, Younggyo Seo, et al. **Object-Aware Regularization for Addressing Causal Confusion in Imitation Learning**. NeurIPS 2021. [[Paper]](https://arxiv.org/abs/2110.14118) [[Code]](https://github.com/alinlab/oreo)
2. Mingzhu Shen, Feng Liang, et al. **Once Quantization-Aware Training: High Performance Extremely Low-bit Architecture Search**. ICCV 2021. [[Paper]](https://arxiv.org/abs/2010.04354) [[Code]](https://github.com/LaVieEnRoseSMZ/OQA)
#### 2022
1. Diego Paez-Granados, Yujie He, et al. **Pedestrian-Robot Interactions on Autonomous Crowd Navigation: Reactive Control Methods and Evaluation Metrics**. IROS 2022. [[Paper]](https://arxiv.org/abs/2208.02121) [[Code]](https://github.com/epfl-lasa/crowdbot-evaluation-tools)
2. Guang Yang, Juan Cao, et al. **DRAG: Dynamic Region-Aware GCN for Privacy-Leaking Image Detection**. AAAI 2022. [[Paper]](https://arxiv.org/abs/2203.09121) [[Code]](https://github.com/guang-yanng/drag)
#### 2023
1. Zhiyang Guo, Wengang Zhou, et al. **HandNeRF: Neural Radiance Fields for Animatable Interacting Hands**. CVPR 2023. [[Paper]](https://arxiv.org/abs/2303.13825) 
2. Ryan K. Cosner, Yuxiao Chen, et al. **Learning Responsibility Allocations for Safe Human-Robot Interaction with Applications to Autonomous Driving**. ICRA 2023. [[Paper]](https://arxiv.org/abs/2303.03504) [[Code]](https://github.com/rkcosner/learning_responsibility_allocation)
3. Catarina G. Fidalgo, Maur√≠cio Sousa, et al. **MAGIC: Manipulating Avatars and Gestures to Improve Remote Collaboration**. VR 2023. [[Paper]](https://arxiv.org/abs/2302.07909) 
4. Omid Mohaddesi, Noah Chicoine, et al. **Thought Bubbles: A Proxy into Players' Mental Model Development**. CHI 2023. [[Paper]](https://arxiv.org/abs/2301.13101) 
5. Yuchen Cui, Siddharth Karamcheti, et al . **"No, to the Right" -- Online Language Corrections for Robotic Manipulation via Shared Autonomy**. HRI 2023. [[Paper]](https://arxiv.org/abs/2301.02555) [[Code]](https://github.com/stanford-iliad/lilac)

## AR-based human-robot interaction<br>
#### 2022
1.  David Puljiz, Bj√∂rn Hein. **Updating Industrial Robots for Emerging Technologies**. HRI 2022.[[Paper]](https://arxiv.org/abs/2204.03538) 
2.  Ryo Suzuki, Adnan Karim, et al. **Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces**. 	CHI 2022. [[Paper]](https://arxiv.org/abs/2203.03254)
#### 2023
1. Joya Chen, Difei Gao, et al. **Affordance Grounding from Demonstration Video to Target Image**. 	CVPR 2023. [[Paper]](https://arxiv.org/abs/2303.14644) [[Code]](https://github.com/showlab/afformer)
2. Theodora Kontogianni, Ekin Celikkan, et al. **Interactive Object Segmentation in 3D Point Clouds**. ICRA 2023. [[Paper]](https://arxiv.org/abs/2204.07183)

## human-robot trust interaction<br>
#### 2020
1. Zahra Rezaei Khavas, Reza Ahmadzadeh, et al. **Modeling Trust in Human-Robot Interaction: A Survey**. ICSR 2020. [[Paper]](https://arxiv.org/abs/2011.04796)
2. Tom Weber, Stefan Wermter. **Integrating Intrinsic and Extrinsic Explainability: The Relevance of Understanding Neural Networks for Human-Robot Interaction**.  AAAI 2020. [[Paper]](https://arxiv.org/abs/2010.04602)
#### 2021
1. Brian J. Zhang, Knut Peterson,  et al. **Exploring Consequential Robot Sound: Should We Make Robots Quiet and Kawaii-et?**. IROS 2021. [[Paper]](https://arxiv.org/abs/2104.02191)
#### 2022
1. Maia Stiber, Russell Taylor, et al. **Modeling Human Response to Robot Errors for Timely Error Detection**. IROS 2022. [[Paper]](https://arxiv.org/abs/2208.00565) 
2. Takane Ueno, Yuto Sawa, et al. **Trust in Human-AI Interaction: Scoping Out Models, Measures, and Methods**. CHI 2022. [[Paper]](https://arxiv.org/abs/2205.00189)
3. Brian Tang, Dakota Sullivan, et al. **CONFIDANT: A Privacy Controller for Social Robots**. HRI 2022. [[Paper]](https://arxiv.org/abs/2201.02712)

## human-robot-avatar interaction (in the Metaverse)<br>
#### 2021
1. Max Schwarz, Christian Lenz, et al. **NimbRo Avatar: Interactive Immersive Telepresence with Force-Feedback Telemanipulation**. IROS 2021. [[Paper]](https://arxiv.org/abs/2109.13772)
#### 2023
1. Purva Tendulkar, D√≠dac Sur√≠s, et al. **FLEX: Full-Body Grasping Without Full-Body Grasps**. CVPR 2023. [[Paper]](https://arxiv.org/abs/2211.11903)
